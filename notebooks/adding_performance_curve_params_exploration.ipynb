{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beca554e-e642-4183-8211-c43676876655",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from dmlutils.building_upgrades import upgrades\n",
    "from dmlutils.gcs import save_fig_to_gcs\n",
    "\n",
    "import src.globals as g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c025a9-0416-4790-a4ba-34e38fdfcb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load test set error metrics and aggregated error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458c8bbc-81a5-415e-ada0-df2ec4eb676d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "building_upgrade_zone_type = (\n",
    "    spark.table(f\"{g.BUILDING_FEATURE_TABLE}_{g.CURRENT_VERSION_NUM}\")\n",
    "    .withColumn(\"zone_type\", F.regexp_extract(\"hvac_heating_efficiency\", r\"(MZ|SZ)\", 0))\n",
    "    .select(\"building_id\", \"upgrade_id\", \"zone_type\")\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f70ace8-476c-41d0-a793-dae03c0bd01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "g.CURRENT_VERSION_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527056d3-1e23-4532-9723-b0c88613d246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(str(g.GCS_CURRENT_VERSION_ARTIFACT_PATH / \"prediction_metrics_test_set.csv\")).query(\n",
    "    \"fuel == 'total'\"\n",
    ")[[\"building_id\", \"upgrade_id\", \"actual\", \"prediction\"]]\n",
    "\n",
    "predictions = pd.read_csv(\n",
    "    str(g.GCS_ARTIFACT_PATH / \"oem_hp_only_experiment\" / \"prediction_metrics_test_set.csv\")\n",
    ").query(\"fuel == 'total'\")[[\"building_id\", \"upgrade_id\", \"actual\", \"prediction\"]]\n",
    "\n",
    "predictions = predictions.merge(building_upgrade_zone_type, on=[\"upgrade_id\", \"building_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0c638c-1c13-44e6-99b6-12ad33586fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predictions_all_upgrade_ids = predictions.drop(\"prediction\", axis=1).merge(\n",
    "    predictions.drop(\"actual\", axis=1), on=[\"building_id\", \"zone_type\"], suffixes=[\"\", \"_predicted\"]\n",
    ")\n",
    "\n",
    "# Filter the data as you originally did\n",
    "df_filtered = (\n",
    "    predictions_all_upgrade_ids.groupby([\"upgrade_id\", \"zone_type\"])\n",
    "    .apply(\n",
    "        lambda group: group[\n",
    "            (group[\"actual\"] > group[\"actual\"].quantile(0.05))\n",
    "            & (group[\"actual\"] < group[\"actual\"].quantile(0.95))  # Remove bottom 5%  # Remove top 5%\n",
    "        ]\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Create the lmplot\n",
    "g = sns.lmplot(\n",
    "    x=\"actual\",\n",
    "    y=\"prediction\",\n",
    "    hue=\"upgrade_id_predicted\",\n",
    "    col=\"upgrade_id\",\n",
    "    row=\"zone_type\",\n",
    "    data=df_filtered,\n",
    "    facet_kws={\"sharex\": False, \"sharey\": False},\n",
    "    scatter_kws={\"s\": 0.05, \"alpha\": 0.2},\n",
    ")\n",
    "\n",
    "# Add a y=x line to each facet\n",
    "for ax in g.axes.flat:\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), color=\"black\", ls=\"--\", label=\"y=x\", lw=3)\n",
    "\n",
    "# Create a color palette for the hue categories\n",
    "palette = sns.color_palette(\"deep\", n_colors=len(df_filtered[\"upgrade_id_predicted\"].unique()))\n",
    "\n",
    "# Customize the legend to use solid color patches\n",
    "handles = [\n",
    "    mlines.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=palette[i], markersize=10, label=str(hue))\n",
    "    for i, hue in enumerate(df_filtered[\"upgrade_id_predicted\"].unique())\n",
    "]\n",
    "\n",
    "# Remove the default legend and add the custom one\n",
    "g._legend.remove()\n",
    "g.fig.legend(handles=handles, title=\"Upgrade ID Predicted\", loc=\"center\", bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310d7913-9a53-45e0-99a4-dc9192bc38e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.13.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017ae6e4-3902-47a7-b87f-070534d9355d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# NOTE: for now this cannot depend on dmlutils because training/testing requires a GPU cluster\n",
    "# and we do not yet have a requirements file that is compatible for both dmlutils and the GPU cluster\n",
    "# from dmlutils.gcs import save_fig_to_gcs\n",
    "\n",
    "from src.globals import GCS_CURRENT_VERSION_ARTIFACT_PATH\n",
    "from src.datagen import DataGenerator, load_data\n",
    "from src.surrogate_model import SurrogateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d275639f-ddb5-47ef-b9a9-78544b91e7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "_, _, test_data = load_data(n_test=100)\n",
    "# init data generator so we can get all of the features -- note that we can't use databricks automatic lookup of features since we logged with mlflow\n",
    "test_gen = DataGenerator(test_data)\n",
    "# reload the training set but with building id and upgrade id keys which we need (this is a little hacky..)\n",
    "test_set = test_gen.init_training_set(train_data=test_data, exclude_columns=[\"weather_file_city\"]).load_df()\n",
    "# convert from pyspark to pandas so we can run inference\n",
    "inference_data = test_set.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce196c53-c153-44ed-9f96-7152e7bef0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# init model\n",
    "sm = SurrogateModel()\n",
    "# mlflow.pyfunc.get_model_dependencies(model_uri=sm.get_model_uri(run_id=RUN_ID))\n",
    "# Load the unregistered model using run ID\n",
    "model = mlflow.pyfunc.load_model(model_uri=sm.get_model_uri(run_id=\"af6dd687ec494a97a7ab32fcc542f67e\"))  # without perf\n",
    "model_w_perf_metrics = mlflow.pyfunc.load_model(\n",
    "    model_uri=sm.get_model_uri(run_id=\"02ef6330c3d54e27b3a0c9442ac70309\")\n",
    ")  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ffdd57-533d-4c84-9ad4-5ff871939b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_feature_impact(model, inference_data, feature_name, grid_values=None, num_points=50):\n",
    "    \"\"\"\n",
    "    Analyze how model predictions change when varying one feature (batch version).\n",
    "\n",
    "    Args:\n",
    "        model: Loaded TensorFlow model\n",
    "        inference_data: Original DataFrame with samples\n",
    "        feature_name: Name of feature to vary\n",
    "        grid_values: Specific values to test (optional)\n",
    "        num_points: Number of points if grid_values not provided\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with results and displays a plot\n",
    "    \"\"\"\n",
    "    # Create a copy of the original data to avoid modifying it\n",
    "    original_data = inference_data.copy()\n",
    "\n",
    "    # If grid values not provided, create a linear space\n",
    "    if grid_values is None:\n",
    "        min_val = original_data[feature_name].min()\n",
    "        max_val = original_data[feature_name].max()\n",
    "        grid_values = np.linspace(min_val, max_val, num_points)\n",
    "\n",
    "    # Create all modified datasets at once\n",
    "    batch_data = pd.concat([original_data.assign(**{feature_name: value}) for value in grid_values], ignore_index=True)\n",
    "\n",
    "    # Add identifier for which grid value each row belongs to\n",
    "    batch_data[\"_grid_value\"] = np.repeat(grid_values, len(original_data))\n",
    "\n",
    "    # Run single batch prediction\n",
    "    predictions = np.nansum(model.predict(batch_data), axis=1)\n",
    "\n",
    "    # Add predictions to the batch data\n",
    "    batch_data[\"_prediction\"] = predictions\n",
    "\n",
    "    # Group by grid value and calculate statistics\n",
    "    results = (\n",
    "        batch_data.groupby(\"_grid_value\")[\"_prediction\"].agg([\"mean\", \"median\", \"std\", \"min\", \"max\"]).reset_index()\n",
    "    )\n",
    "    results.columns = [\n",
    "        \"feature_value\",\n",
    "        \"mean_prediction\",\n",
    "        \"median_prediction\",\n",
    "        \"std_prediction\",\n",
    "        \"min_prediction\",\n",
    "        \"max_prediction\",\n",
    "    ]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results[\"feature_value\"], results[\"mean_prediction\"], label=\"Mean Prediction\", color=\"blue\")\n",
    "    plt.fill_between(\n",
    "        results[\"feature_value\"],\n",
    "        results[\"mean_prediction\"] - results[\"std_prediction\"],\n",
    "        results[\"mean_prediction\"] + results[\"std_prediction\"],\n",
    "        alpha=0.2,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel(\"Prediction Sum\")\n",
    "    plt.title(f\"Impact of {feature_name} on Model Predictions (Batch)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ee5429-5095-4926-899c-9c591aff2094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_feature_impact(\n",
    "    model=model, inference_data=inference_data, feature_name=\"heating_efficiency_nominal_percentage\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839d4178-018e-4706-8c69-ea5f1f7c9a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Or with specific grid values:\n",
    "# custom_grid = np.linspace(3, 4, 4)  # from 0 to 1 in 20 steps\n",
    "results = analyze_feature_impact(\n",
    "    model=model, inference_data=inference_data, feature_name=\"heating_efficiency_nominal_percentage\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2943aa8-dd55-459d-9c0a-8b1456dc4e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Or with specific grid values:\n",
    "custom_grid = np.linspace(19, 23, 4)  # from 0 to 1 in 20 steps\n",
    "results = analyze_feature_impact(\n",
    "    model=model, inference_data=inference_data, feature_name=\"cooling_efficiency_eer\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396792c7-8c29-4ab8-b6ed-6e0ae47f4bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Or with specific grid values:\n",
    "custom_grid = np.linspace(3, 4, 4)  # from 0 to 1 in 20 steps\n",
    "results = analyze_feature_impact(\n",
    "    model=model_w_perf_metrics,\n",
    "    inference_data=inference_data,\n",
    "    feature_name=\"heating_efficiency_nominal_percentage\",\n",
    "    num_points=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2506a5-eb34-4012-8dcb-792e7bc6b299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_grid = np.linspace(19, 23, 4)  # from 0 to 1 in 20 steps\n",
    "results = analyze_feature_impact(\n",
    "    model=model_w_perf_metrics, inference_data=inference_data, feature_name=\"cooling_efficiency_eer\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc656d1-5506-4b02-868d-e033ac18a791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_grid = np.linspace(0, 6, 10)  # from 0 to 1 in 20 steps\n",
    "results = analyze_feature_impact(\n",
    "    model=model_w_perf_metrics, inference_data=inference_data, feature_name=\"min_cop_5f\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c4e049-893d-49f8-84d7-4e0c8fc8b349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = analyze_feature_impact(\n",
    "    model=model_w_perf_metrics, inference_data=inference_data, feature_name=\"min_capacity_5f\", num_points=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb456266-515b-4550-9caf-4d68e6c8d070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(f\"{g.BUILDING_FEATURE_TABLE}_{g.CURRENT_VERSION_NUM}\").withColumn(\n",
    "    \"zone_type\", F.regexp_extract(\"hvac_heating_efficiency\", r\"(MZ|SZ)\", 0)\n",
    ").select(\"min_cop_47f\", \"upgrade_id\", \"zone_type\").distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f87c0f5-8d2a-489b-8e9b-97c5132ad565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "adding_performance_curve_params_exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
